#!/usr/bin/env python3

"""
Test Full Twitter Integration Pipeline:
Twitter Fetcher ‚Üí Redis Stream ‚Üí Worker ‚Üí LLM Content Analyzer ‚Üí Pinecone
"""

import asyncio
import logging
import sys
import os
import time

# Add backend to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

async def test_full_pipeline():
    """Test complete pipeline from Twitter to Pinecone"""
    print("üöÄ –ü–û–õ–ù–ê–Ø –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø: Twitter ‚Üí Redis ‚Üí Worker ‚Üí LLM Analyzer ‚Üí Pinecone")
    print("=" * 80)
    
    # Step 1: Run Twitter Fetcher
    print("\nüì° –®–∞–≥ 1: –ó–∞–ø—É—Å–∫ Twitter Fetcher...")
    try:
        from fetchers.twitter_fetcher import main_twitter_task
        await main_twitter_task(mode="experts", hours_back=24)
        print("‚úÖ Twitter Fetcher –∑–∞–≤–µ—Ä—à–µ–Ω - –¥–∞–Ω–Ω—ã–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –≤ Redis Stream")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Twitter Fetcher: {e}")
        return False
    
    # Step 2: Check Redis Stream
    print("\nüìä –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ Redis Stream...")
    try:
        import redis
        redis_client = redis.from_url(os.getenv("REDIS_URL", "redis://localhost:6379/0"))
        
        # Check stream length
        stream_length = redis_client.xlen("stream:raw_events")
        print(f"üìã Events –≤ stream: {stream_length}")
        
        if stream_length == 0:
            print("‚ö†Ô∏è –ù–µ—Ç —Å–æ–±—ã—Ç–∏–π –≤ stream - –≤–æ–∑–º–æ–∂–Ω–æ Twitter Fetcher –Ω–µ –Ω–∞—à–µ–ª –Ω–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤")
            return False
        
        # Show recent events
        recent_events = redis_client.xrevrange("stream:raw_events", count=3)
        print("üìù –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è:")
        for event_id, event_data in recent_events:
            event_type = event_data.get(b'event_type', b'unknown').decode('utf-8')
            source = event_data.get(b'source', b'unknown').decode('utf-8')
            print(f"  - {event_id.decode()}: {event_type} from {source}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Redis: {e}")
        return False
    
    # Step 3: Process events with Worker
    print("\n‚öôÔ∏è –®–∞–≥ 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π Worker...")
    try:
        from jobs.worker import consume_raw_events_stream, setup_streams
        
        # Setup streams
        setup_streams()
        
        # Process some events
        events_processed = 0
        for _ in range(3):  # Try 3 times
            processed = await consume_raw_events_stream()
            if processed:
                events_processed += 1
                print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ {events_processed}: –°–æ–±—ã—Ç–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
            else:
                break
        
        if events_processed == 0:
            print("‚ö†Ô∏è Worker –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–ª –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å–æ–±—ã—Ç–∏—è")
            return False
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Worker: {e}")
        return False
    
    # Step 4: Check Pinecone updates
    print("\nüîç –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π Pinecone...")
    try:
        from pinecone import Pinecone
        
        pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        index = pc.Index(os.getenv("PINECONE_INDEX", "mrbets-content-chunks"))
        
        # Get current stats
        stats = index.describe_index_stats()
        total_vectors = stats.total_vector_count
        
        print(f"üìä –í—Å–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ Pinecone: {total_vectors}")
        
        # Query for recent Twitter content
        dummy_vector = [0.0] * 1536
        twitter_results = index.query(
            vector=dummy_vector,
            filter={"source": {"$eq": "twitter"}},
            top_k=5,
            include_metadata=True
        )
        
        twitter_count = len(twitter_results.matches)
        print(f"üê¶ Twitter –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ Pinecone: {twitter_count} –≤–µ–∫—Ç–æ—Ä–æ–≤")
        
        if twitter_count > 0:
            print("üìù –ü—Ä–∏–º–µ—Ä—ã Twitter –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
            for i, match in enumerate(twitter_results.matches[:3]):
                metadata = match.metadata
                print(f"  {i+1}. {metadata.get('document_title', 'Unknown')}")
                print(f"     Score: {match.score:.3f}, Type: {metadata.get('chunk_type', 'unknown')}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Pinecone: {e}")
        return False
    
    # Step 5: Check Supabase processed_documents
    print("\nüìö –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ Supabase processed_documents...")
    try:
        from supabase import create_client
        
        supabase = create_client(
            os.getenv("SUPABASE_URL"), 
            os.getenv("SUPABASE_SERVICE_KEY")
        )
        
        # Check recent Twitter documents
        response = supabase.table("processed_documents").select(
            "source, document_title, document_timestamp"
        ).eq("source", "twitter").order("document_timestamp", desc=True).limit(5).execute()
        
        twitter_docs = len(response.data) if response.data else 0
        print(f"üê¶ Twitter –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Supabase: {twitter_docs}")
        
        if twitter_docs > 0:
            print("üìù –ü–æ—Å–ª–µ–¥–Ω–∏–µ Twitter –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
            for i, doc in enumerate(response.data[:3]):
                print(f"  {i+1}. {doc['document_title']}")
                print(f"     Timestamp: {doc['document_timestamp']}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Supabase: {e}")
        return False
    
    return True

async def main():
    """Main test function"""
    print("üß™ –¢–ï–°–¢ –ü–û–õ–ù–û–ô –ò–ù–¢–ï–ì–†–ê–¶–ò–ò TWITTER PIPELINE")
    print("=" * 80)
    
    start_time = time.time()
    success = await test_full_pipeline()
    duration = time.time() - start_time
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò–ù–¢–ï–ì–†–ê–¶–ò–û–ù–ù–û–ì–û –¢–ï–°–¢–ê")
    print("=" * 80)
    print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f}s")
    
    if success:
        print("üéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´! Twitter –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é!")
        print("")
        print("üîÑ –ü–æ–ª–Ω—ã–π pipeline —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç:")
        print("   üì° Twitter API ‚Üí üì® Redis Stream ‚Üí ‚öôÔ∏è Worker ‚Üí üß† LLM Analyzer ‚Üí üîç Pinecone")
        print("")
        print("‚úÖ Twitter –∫–æ–Ω—Ç–µ–Ω—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:")
        print("   - –°–æ–±–∏—Ä–∞–µ—Ç—Å—è –æ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤")
        print("   - –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é LLM")
        print("   - –†–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —É–º–Ω—ã–µ —á–∞–Ω–∫–∏")
        print("   - –õ–∏–Ω–∫—É–µ—Ç—Å—è —Å –∫–æ–º–∞–Ω–¥–∞–º–∏/–∏–≥—Ä–æ–∫–∞–º–∏")
        print("   - –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ embeddings")
        print("   - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ Pinecone –¥–ª—è –ø–æ–∏—Å–∫–∞")
        print("")
        print("üöÄ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ø–æ–ª–Ω—è–µ—Ç—Å—è!")
    else:
        print("‚ùå –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —ç—Ç–∞–ø—ã –Ω–µ –ø—Ä–æ—à–ª–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é.")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:")
        print("   - Redis –∑–∞–ø—É—â–µ–Ω")
        print("   - API –∫–ª—é—á–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
        print("   - –ï—Å—Ç—å —Å–≤–µ–∂–∏–µ —Ç–≤–∏—Ç—ã –æ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤")

if __name__ == "__main__":
    asyncio.run(main()) 