# ğŸ“˜ CONTEXT.md â€” Project Overview for MrBets.ai
_Last updated: 2025-04-29_

---

## ğŸ¯ Project Evolution: MrBets.ai (Formerly WinPulse)

MrBets.ai is an AI-powered sports prediction platform that delivers pre-analyzed football match insights based on comprehensive data collection, advanced AI reasoning, and betting value assessment.

The project is evolving from a Next.js-only application with serverless functions to a robust monorepo architecture with a dedicated FastAPI backend for enhanced data processing capabilities.

---

## ğŸ—ï¸ Architecture Overview

### Current Architecture
- **Frontend**: Next.js 14 with App Router deployed on Vercel
- **Database**: Supabase (PostgreSQL)
- **API Integrations**: API-Football, OpenAI
- **Automation**: Vercel Cron Jobs

### Planned Architecture
- **Frontend**: Next.js 14 (unchanged)
- **Backend**: FastAPI with microservices architecture
- **Data Processing**: Redis Streams for distributed processing
- **Vector Search**: Pinecone for semantic retrieval
- **Storage**: Supabase Storage for raw data
- **Additional Data Sources**: News sites, social media, video content

---

## ğŸ§© Key Functional Components

### 1. ğŸ”„ **Fixtures Sync**
- Uses `API-Football` to fetch upcoming matches from select leagues (e.g., Premier League, La Liga)
- **Current**: Script: `/scripts/updateFixtures.ts`
- **Planned**: Python service: `/backend/jobs/scan_fixtures.py`
- Stores match data into `Supabase.fixtures`
- Triggered via cron job

### 2. ğŸ§  **AI Prediction Generation**
- Each match is processed by:
  - Fetching live stats & odds
  - Formatting a structured prompt
  - Sending to OpenAI (model: `o4-mini`, planned upgrade to `GPT-4o`)
  - Parsing AI response into:
    - `CHAIN OF THOUGHT`
    - `FINAL PREDICTION`
    - `VALUE BETS`
- **Current**: Script: `/scripts/generatePrediction.ts`
- **Planned**: Pipeline: 
  - Data collection via various fetchers
  - Pre-processing via `/backend/processors/pre_processor.py`
  - Retrieval via `/backend/processors/retriever_builder.py`
  - LLM reasoning via `/backend/processors/llm_reasoner.py`
- All results are stored in `Supabase.ai_predictions`

### 3. ğŸ“… **Automated Processing**
- **Current**: Nightly cron job (4:00 AM UTC) runs two steps:
  - `run-update.js` â†’ Updates fixtures
  - `run-all-predictions.js` â†’ Generates predictions for unprocessed matches
  - Cron route: `/api/cron/daily-update`
  - Deployment: Vercel with `vercel.json` cron support
- **Planned**: Continuous processing:
  - Data collection every 30 minutes via cron
  - Redis Streams for distributed processing
  - Auto-refresh when significant changes detected (odds shifts, new content)

### 4. ğŸ’¬ **User-Facing /ai Page**
- Users visit `/ai`
- See horizontally scrollable cards of upcoming matches
- Clicking a card:
  - Opens the prediction display
  - Loads prediction from `Supabase.ai_predictions`
  - Displays:
    - Chain of Thought
    - Final Prediction
    - List of Top 3 Value Bets
- If no prediction exists:
  - Fallback message: "AI prediction is being prepared..."

### 5. ğŸ” **Auth & Visibility**
- Top matches in `/ai` preview are blurred unless the user logs in
- Login is email-based (OTP) using Supabase Auth
- After login, full dashboard unlocked

---

## ğŸ”’ Database Structure (Supabase)

### Current Tables

#### `fixtures`
- `fixture_id`: integer (primary key)
- `league_id`: integer
- `home_id`: integer
- `away_id`: integer
- `utc_kickoff`: timestamp
- `status`: text
- `score_home`: integer
- `score_away`: integer
- `last_updated`: timestamp

#### `ai_predictions`
- `id`: uuid (primary key)
- `fixture_id`: integer (foreign key)
- `type`: text (e.g., "pre-match")
- `chain_of_thought`: text
- `final_prediction`: text
- `value_bets_json`: string (stringified array)
- `model_version`: text
- `generated_at`: timestamp
- `stale`: boolean (default: false)

### Planned Additional Tables

#### `raw_events`
- `id`: uuid (primary key)
- `match_id`: integer (foreign key)
- `source`: text (e.g., "twitter", "bbc", "youtube")
- `content_hash`: text (for deduplication)
- `content_path`: text (path in Supabase Storage)
- `metadata`: jsonb
- `reliability_score`: float
- `created_at`: timestamp

#### `emb_cache`
- `hash`: text (primary key)
- `vector`: vector (1536d)
- `created_at`: timestamp

---

## ğŸ“¡ External APIs & Models

### Current Integrations

#### âœ… API-Football
- Used for: Fixtures, live odds, team data, predictions
- Called via `lib/apiFootball.ts`
- Rate-limited API with key-based access

#### âœ… OpenAI
- Used for: Text generation (reasoning, summary, betting logic)
- Model: `o4-mini`
- Called via `generatePrediction.ts` â†’ OpenAI Chat Completion endpoint
- Responses are split into 3 sections and stored

### Planned Integrations

#### â³ OpenAI Embeddings
- Use: Vectorizing text for semantic search
- Model: `text-embedding-3-large`
- Will be called via: `/backend/processors/pre_processor.py`

#### â³ Pinecone
- Use: Vector database for similar document retrieval
- Will store all processed text embeddings
- Will be queried for context retrieval for each match

#### â³ BeautifulSoup + Requests
- Use: Scraping news sites and sports blogs
- Will extract clean text from HTML

#### â³ DeepL API
- Use: Translation services for non-English content
- Will ensure all analysis happens on English text

#### â³ Whisper
- Use: Transcription of YouTube videos and podcasts
- Model: medium or large-v3
- Will convert audio analysis to searchable text

#### â³ Twitter/Telegram APIs
- Use: Gathering expert opinions and community insights
- Will capture real-time discussions about upcoming matches

---

## ğŸ“Š Data Flow Pipeline (Planned)

```
[CRON] â†’ scan_fixtures.py â†’ redis:queue:fixtures (LIST) â†’ worker_manager
                                       â”‚ POP
                                       â–¼
                   Â«data-collector workflowÂ» (fan-out 5 fetcher tasks)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rest_fetcher.py (API-Football)       â†’ raw_events (STREAM)          â”‚
â”‚ scraper_fetcher.py (BBC/ESPN)        â†’ raw_events (STREAM)          â”‚
â”‚ twitter_fetcher.py (filtered stream) â†’ raw_events (STREAM)          â”‚
â”‚ telegram_fetcher.py (bot getUpdates) â†’ raw_events (STREAM)          â”‚
â”‚ youtube_fetcher.py (yt-dlp+Whisper)  â†’ raw_events (STREAM)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼ consumer-group:pre_processor (parallel workers)
     translate â†’ split_chunks â†’ reliability_score â†’ embeddings â†’ Storage/Pinecone/Postgres
                     â–¼
           retriever_builder.py (sql filter + pinecone topK + dedup)
                     â–¼
           llm_reasoner.py (prompt + live odds â†’ GPT-4o) â†’ JSON
                     â–¼
           result_writer.py (UPSERT predictions, TTL 3h)
```

This pipeline will run automatically every 30 minutes, constantly updating the data store with fresh information and generating new predictions when needed.

---

## ğŸ§ª Development Environment

### Current Environment
- Project is developed on Windows 11 + PowerShell
- Local script execution via `ts-node`
- Supabase MCP enabled for Cursor integration
- `.env` stores all API keys & model info

### Planned Environment
- Development on Windows 11 with WSL 2
- Docker + docker-compose for full system simulation
- Backend development in Python 3.11
- Frontend continues with Next.js/TypeScript
- Environment variables split between frontend and backend needs

---

## ğŸ’» Repository Structure (Planned)

```
mrbets/                         # Repository name
â”œâ”€â”€ frontend/                    # Current Next.js project
â”‚   â”œâ”€â”€ src/                     # Frontend source code
â”‚   â”œâ”€â”€ public/                  # Static files 
â”‚   â”œâ”€â”€ package.json             # Frontend dependencies
â”‚   â””â”€â”€ ...                      # Other Next.js files
â”‚
â”œâ”€â”€ backend/                     # New FastAPI backend
â”‚   â”œâ”€â”€ app/                     # Main FastAPI code
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI entry point
â”‚   â”‚   â”œâ”€â”€ routers/             # API routes
â”‚   â”‚   â””â”€â”€ models/              # Data schemas
â”‚   â”œâ”€â”€ jobs/                    # Scripts for cron tasks
â”‚   â”‚   â”œâ”€â”€ scan_fixtures.py
â”‚   â”‚   â””â”€â”€ worker.py
â”‚   â”œâ”€â”€ fetchers/                # Data collection microservices
â”‚   â”œâ”€â”€ processors/              # Data processors
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml           # For running the entire system
â”œâ”€â”€ .github/                     # GitHub Actions
â”œâ”€â”€ BACKEND.md                   # Backend documentation
â””â”€â”€ CONTEXT.md                   # This file
```

---

## ğŸ”œ Project Timeline (May 9 â†’ August 9)

| Period | Goal | Deliverables |
|--------|------|--------------|
| May 09â€“15 | Dev environment + CI | docker-compose, GitHub Actions |
| May 16â€“22 | Queue demo | fixtures-scanner, Redis with tasks |
| May 23â€“29 | REST/Scraper fetchers | BBC + API-Football data in DB |
| May 30â€“Jun 05 | Whisper pipeline | YouTube texts in Storage/Postgres |
| Jun 06â€“12 | Pre-processor | Translation, chunks, embeddings |
| Jun 13â€“19 | Vector search | Pinecone Top-K API ready |
| Jun 20â€“26 | Retriever API | `/context/{match_id}` returns facts |
| Jun 27â€“Jul 03 | LLM draft | First JSON prediction saved |
| Jul 04â€“10 | TTL cache | Reading `predictions` on frontend |
| Jul 11â€“17 | Odds module | Live odds in prompt |
| Jul 18â€“24 | Frontend integration | Next.js match page shows prediction |
| Jul 25â€“31 | Affiliate links | Smart links + GTM events |
| Aug 01â€“07 | Load testing | 100 matches, <2 sec latency |
| Aug 08â€“09 | UI polish and release | Landing, animation, prod deploy |

---

## ğŸ”‘ Key Terms

| Term | What it is | In simple words |
|------|------------|----------------|
| **Redis** | In-memory data store | Ultra-fast task queue between services |
| **Redis Streams** | Message broker pattern | Ordered message queue with consumer groups |
| **Embeddings** | Text vectorization | Representing text meaning as numbers for search |
| **Hybrid Search** | Filter + vector search | First by team/date, then by meaning |
| **TTL** | Time To Live | Prediction validity period (3 hours) |
| **Chain-of-Thought** | Prompting method | LLM explicitly shows reasoning process |
| **Pinecone** | Vector database | Stores and searches semantically similar texts |
| **Whisper** | Speech recognition model | Converts YouTube audio to text |
| **FastAPI** | Python web framework | High-performance API with automatic documentation |
| **BeautifulSoup** | HTML parser | Extracts text from web pages |

---

## ğŸ“‹ First Tasks (By May 9)

1. Set up monorepo structure (frontend/ and backend/ directories)
2. Create docker-compose.yml for three services (frontend, backend, redis)
3. Install WSL 2 and Docker Desktop on Windows
4. Set up Supabase project with initial tables
5. Create `.env.example` template with all required API keys
6. Implement `scan_fixtures.py` script for fetching matches
7. Create demo worker to validate Redis queue functionality
8. Ensure documentation (BACKEND.md) is up to date

---

This file is the **single source of truth** for how the platform is designed to work.  
Keep it updated when functionality evolves.

## ğŸ“ Update: 2025-05-10 - ĞœĞ¾Ğ½Ğ¾Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ FastAPI Backend

Ğ ĞµĞ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ² Ğ¼Ğ¾Ğ½Ğ¾Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ FastAPI Ğ±ÑĞºĞµĞ½Ğ´Ğ¾Ğ¼:

### 1. ĞĞ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°

```
mrbets/
â”œâ”€â”€ frontend/            # Next.js frontend
â”‚   â”œâ”€â”€ src/             # React ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
â”‚   â”œâ”€â”€ public/          # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹
â”‚   â””â”€â”€ package.json     # Frontend Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
â”‚
â”œâ”€â”€ backend/             # FastAPI backend
â”‚   â”œâ”€â”€ app/             # FastAPI ĞºĞ¾Ğ´
â”‚   â”‚   â”œâ”€â”€ main.py      # ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ²Ñ…Ğ¾Ğ´Ğ°
â”‚   â”‚   â”œâ”€â”€ routers/     # API Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ñ‹
â”‚   â”‚   â”œâ”€â”€ models/      # Ğ¡Ñ…ĞµĞ¼Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 
â”‚   â”‚   â””â”€â”€ utils/       # Ğ£Ñ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹
â”‚   â”œâ”€â”€ jobs/            # Cron Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸
â”‚   â”‚   â”œâ”€â”€ scan_fixtures.py
â”‚   â”‚   â””â”€â”€ worker.py
â”‚   â”œâ”€â”€ fetchers/        # Ğ¡Ğ±Ğ¾Ñ€Ñ‰Ğ¸ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
â”‚   â”œâ”€â”€ processors/      # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
â”‚   â””â”€â”€ requirements.txt # Backend Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
â”‚
â”œâ”€â”€ monitoring/          # Prometheus Ğ¸ Grafana
â”œâ”€â”€ docker-compose.yml   # Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²ÑĞµĞ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
â””â”€â”€ .env.example         # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
```

### 2. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ±ÑĞºĞµĞ½Ğ´Ğ° (FastAPI)

- **FastAPI App**: REST API Ñ ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¼Ğ°Ñ‚Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ°Ñ…
- **Fixture Router**: `/fixtures` ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¼Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸
- **Worker Process**: ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸ Redis
- **Scan Fixtures Job**: ĞŸĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ‡ĞµĞ¹

### 3. ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°

- **Prometheus**: Ğ¡Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº ÑĞ¾ Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²
- **Grafana**: Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ Ğ°Ğ»ĞµÑ€Ñ‚Ğ¸Ğ½Ğ³

### 4. Docker Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ

- Docker Compose Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²:
  - frontend
  - backend
  - worker
  - redis
  - prometheus
  - grafana

Ğ­Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.

## ğŸ“ Update: 2025-05-10 - ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞœĞ¾Ğ½Ğ¾Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ

Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ Ğ±Ñ‹Ğ»Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¼Ğ¾Ğ½Ğ¾Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ:

### 1. ĞšĞ¾Ñ€Ğ½ĞµĞ²Ğ¾Ğ¹ package.json Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°Ğ¼Ğ¸

Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ ĞºĞ¾Ñ€Ğ½ĞµĞ²Ğ¾Ğ¹ `package.json` Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ğ¼Ğ¸:

```json
{
  "name": "mrbets",
  "version": "1.0.0",
  "scripts": {
    "dev:frontend": "cd frontend && npm run dev",
    "dev:backend": "cd backend && uvicorn app.main:app --reload",
    "dev:all": "concurrently \"npm run dev:frontend\" \"npm run dev:backend\"",
    "build:frontend": "cd frontend && npm run build",
    "start:frontend": "cd frontend && npm run start",
    "docker:up": "docker-compose up -d",
    "docker:down": "docker-compose down",
    "check-monorepo": "bash ./scripts/check-monorepo.sh"
  }
}
```

### 2. ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ .gitignore Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¾Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ

Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½ Ñ„Ğ°Ğ¹Ğ» `.gitignore` Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Python Ğ¸ Node.js:

```
# Python
__pycache__/
*.py[cod]
/backend/.venv/
/backend/*.egg-info/
/backend/.pytest_cache/

# Node.js
/node_modules/
/frontend/node_modules/
/frontend/.next/

# Ğ›Ğ¾Ğ³Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹
*.log
.docker/
dump.rdb
```

### 3. Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹

Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ bash-ÑĞºÑ€Ğ¸Ğ¿Ñ‚ `scripts/check-monorepo.sh` Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ½Ğ¾Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ:
- ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ²ÑĞµÑ… ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
- Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ğ² package.json
- ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
- Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ†Ğ²ĞµÑ‚Ğ½Ğ¾Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°

### 4. ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ

ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ `README.md` Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞµ Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºÑƒ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ½Ğ¾Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ.

Ğ­Ñ‚Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ´Ğ¾Ğ±Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ¼ Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ñ€Ğ¾Ğ½Ñ‚ĞµĞ½Ğ´Ğ° Ğ¸ Ğ±ÑĞºĞµĞ½Ğ´Ğ°.
