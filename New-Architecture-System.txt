Давайте четко по пунктам распишем новую схему и план действий.

**Новая Архитектурная Схема ("Футбольный Эксперт v2.0"):**

**Цель:** Система постоянно и независимо собирает и глубоко осмысливает *весь* релевантный футбольный контент. Когда появляется информация о конкретном матче, система извлекает уже обработанные, релевантные знания для генерации прогноза.

**Основные Этапы Жизненного Цикла Данных и Прогноза:**

1.  **Этап 1: Непрерывный Сбор Сырого Контента (Асинхронно и Независимо)**
    *   **Компоненты:** `Fetcher`-ы (`scraper_fetcher.py`, будущие для Twitter, Telegram, YouTube и т.д.).
    *   **Действие:** Fetcher-ы работают по собственному расписанию (например, каждые 10-15 минут). Они сканируют свои источники (RSS, сайты, соцсети) на наличие **любого нового контента**.
    *   **Выход:** Сырые данные (URL статьи, полный текст, источник, дата публикации) помещаются в Redis Stream `stream:raw_events`. В этих событиях `match_id` будет `None` (или отсутствовать), так как контент еще не привязан к конкретному матчу.

2.  **Этап 2: Глубокое Осмысление и Обогащение Контента (LLM-Powered Preprocessing)**
    *   **Компонент:** `llm_content_analyzer.py` (будет **кардинально** переработан).
    *   **Действие:**
        *   `llm_content_analyzer.py` слушает `stream:raw_events`.
        *   Для каждого нового сырого документа (статьи, поста):
            *   **Дедупликация:** Проверяет, не обрабатывался ли этот контент ранее (по URL или хешу).
            *   **LLM Анализ:** Отправляет текст документа мощной LLM (например, GPT-4o, Claude 3) с детальным промптом. Промпт просит LLM:
                1.  Разбить текст на осмысленные семантические **"умные чанки"**.
                2.  Для **каждого чанка** извлечь: его основную тему/суть, упомянутые команды, игроков, тренеров, турниры (в идеале – их канонические имена), тип информации (факт, мнение, слух, статистика), возможно, тональность и важность.
                3.  Вернуть результат в виде структурированного JSON (массив чанков с их метаданными).
            *   **Обработка ответа LLM:**
                *   Для каждого "умного чанка":
                    *   **Entity Linking (Критично!):** Используя имена команд/игроков из ответа LLM, попытаться сопоставить их с `team_id` и `player_id` из ваших таблиц `teams` и `players` в Supabase. Это может потребовать дополнительной логики на Python (например, поиск по имени, fuzzy matching).
                    *   **Генерация Эмбеддинга:** Создать векторный эмбеддинг для текста чанка (например, через OpenAI `text-embedding-3-small`).
                    *   **Сохранение:**
                        *   **Supabase:** Сохранить каждый "умный чанк" как отдельную запись в новой таблице (например, `content_chunks`). Эта запись будет содержать: текст чанка, метаданные от LLM, связанные `linked_team_ids`, `linked_player_ids`, ID родительского документа, ID эмбеддинга, временные метки.
                        *   **Pinecone:** Сохранить вектор эмбеддинга чанка вместе с его метаданными (`chunk_id`, ID родительского документа, `linked_team_ids`, `linked_player_ids`, источник, дата оригинальной статьи).
    *   **Выход:** База знаний из "умных чанков" контента, обогащенных метаданными и связанных с конкретными командами/игроками (но еще не с `fixture_id`).

3.  **Этап 3: Сканирование Матчей и Инициация Запроса на Прогноз**
    *   **Компонент:** `scan_fixtures.py` (остается похожим).
    *   **Действие:** Работает по расписанию, находит актуальные матчи, сохраняет их в таблицу `fixtures` в Supabase.
    *   **Выход:** Помещает `fixture_id` в очередь Redis `queue:fixtures`. Эта очередь теперь означает: "Для этого матча нужно создать/обновить прогноз".

4.  **Этап 4: Генерация Прогноза для Конкретного Матча**
    *   **Компонент:** `worker.py` (или новый "Prediction Orchestrator").
    *   **Действие:**
        *   Берет `fixture_id` из `queue:fixtures`.
        *   **Retriever (`retriever_builder.py` - новый компонент):**
            *   По `fixture_id` получает `home_team_id` и `away_team_id`.
            *   Запрашивает из Pinecone и Supabase (`content_chunks`) все "умные чанки", которые релевантны этим командам или их игрокам (используя `linked_team_ids`, `linked_player_ids`).
            *   Фильтрует чанки по дате (опубликованные до матча).
            *   Ранжирует, отбирает наиболее важные, возможно, предварительно суммирует их (может быть еще один вызов LLM для "пре-саммари" контекста).
            *   Формирует итоговый набор контекстной информации для матча.
        *   **LLM Reasoner (`llm_reasoner.py` - новый компонент):**
            *   Получает контекст от Retriever-а.
            *   Запрашивает актуальные коэффициенты на матч (из `odds_fetcher.py` или кеша/базы).
            *   Формирует детализированный промпт (с Chain of Thought, структурой Value Bets) для мощной LLM (например, GPT-4o).
            *   Получает JSON с прогнозом, анализом и ставками.
        *   **Result Writer (может быть частью `worker.py` или `llm_reasoner.py`):**
            *   Сохраняет (делает UPSERT) JSON-прогноз в таблицу `ai_predictions` в Supabase, привязывая его к `fixture_id`.
    *   **Выход:** Готовый прогноз в базе данных, доступный для фронтенда.

**План Масштабной Перестройки (Поэтапно):**

Мы не будем переделывать всё сразу. Нужен поэтапный подход.

**Фаза 1: Создание Фундамента для LLM-Управляемого Поглощения Контента**
*(Основной фокус: `scraper_fetcher.py` и полная переработка `llm_content_analyzer.py`)*

1.  **Проектирование Промпта и Структуры Вывода для LLM-Препроцессора:**
    *   **Задача:** Создать промпт, который заставит LLM (например, GPT-4o или Claude 3 Opus/Sonnet) анализировать текст статьи, разбивать его на "умные чанки" и для каждого чанка извлекать необходимые метаданные (тема, сущности, тип информации и т.д.). Определить точный JSON-формат, в котором LLM должен вернуть результат.
    *   **Действие (Совместно):** Мы с вами разработаем этот промпт и JSON-схему.

2.  **Адаптация `scraper_fetcher.py` для Независимой Работы:**
    *   **Задача:** `scraper_fetcher.py` должен запускаться периодически (пока вручную для теста) и собирать *все* новые статьи, отправляя их в `stream:raw_events` с `match_id=None`.
    *   **Изменения:**
        *   `main_scraper_task` больше не принимает `fixture_id` как обязательный аргумент (или `fixture_id=None` по умолчанию).
        *   При отправке события в Redis, поле `match_id` будет `None`.
        *   (В будущем) Добавить дедупликацию на уровне fetcher-а по URL, чтобы не отправлять одну и ту же статью многократно.

3.  **Полная Переработка `llm_content_analyzer.py`:**
    *   **Задача:** `llm_content_analyzer.py` должен брать сырые статьи из `stream:raw_events`, вызывать LLM для их анализа и сохранять структурированные "умные чанки".
    *   **Изменения:**
        *   Удалить старую логику чанкирования (на `tiktoken`) и NER/линковки на `spaCy`.
        *   Реализовать клиент для выбранной LLM (OpenAI, Anthropic и т.д.).
        *   В основной функции обработки события:
            *   Получить текст статьи.
            *   Вызвать LLM с разработанным промптом.
            *   Распарсить JSON-ответ от LLM (список "умных чанков").
            *   **Для каждого "умного чанка":**
                *   Сгенерировать эмбеддинг для текста чанка.
                *   **Реализовать Entity Linking:** На основе имен команд/игроков, полученных от LLM, написать логику для поиска их `team_id`/`player_id` в таблицах Supabase `teams` и `players`. Это очень важный шаг.
                *   **Сохранить чанк:**
                    *   **Supabase:** Создать новую таблицу `content_chunks` (или адаптировать `processed_documents`). Каждая строка — это "умный чанк" с его текстом, метаданными от LLM, `linked_team_ids`, `linked_player_ids`, ID эмбеддинга и т.д. Также нужна таблица `source_documents` для хранения информации об исходной статье.
                    *   **Pinecone:** Сохранить вектор эмбеддинга чанка с его `chunk_id`, `source_document_id`, `linked_team_ids`, `linked_player_ids` и другой релевантной метаинформацией.
        *   Обеспечить обработку ошибок при вызовах LLM API.

4.  **Изменения в Схеме Базы Данных Supabase:**
    *   Создать таблицу `source_documents` (для исходных статей: URL, источник, дата загрузки и т.д.).
    *   Создать таблицу `content_chunks` (для "умных чанков" с их текстом, метаданными от LLM, ID связанных сущностей, ID эмбеддинга, ссылка на `source_documents`).

**Фаза 2: Реализация Retriever-а и Базового Конвейера Генерации Прогнозов**
*(Фокус: `retriever_builder.py`, `llm_reasoner.py`, адаптация `worker.py`)*

5.  **Разработка `retriever_builder.py`:**
    *   **Задача:** По `fixture_id` находить и собирать наиболее релевантные `content_chunks`.
    *   **Логика:** Получить ID команд матча -> Запросить Pinecone/Supabase по `linked_team_ids`/`linked_player_ids` и дате -> Ранжировать и отобрать чанки -> Сформировать контекст.

6.  **Разработка `llm_reasoner.py` (начальная версия):**
    *   **Задача:** На основе контекста от Retriever-а и коэффициентов сгенерировать JSON-прогноз.
    *   **Логика:** Сформировать промпт для GPT-4o (с Chain of Thought и т.д.) -> Отправить запрос -> Получить JSON.

7.  **Адаптация `worker.py` для Оркестровки Прогнозов:**
    *   **Задача:** `worker.py`, получив `fixture_id`, запускает цепочку: Retriever -> Reasoner -> Result Writer.
    *   **Изменения:** Убрать прямой вызов `scraper_fetcher`. Вместо этого вызывать новые компоненты.

**Фаза 3: Настройка Расписаний, Оптимизация и Итерации**

8.  **Независимый Запуск Fetcher-ов:**
    *   Настроить cron-задачи (или использовать APScheduler в отдельном сервисе) для периодического запуска `scraper_fetcher.main_scraper_task()` и других контентных fetcher-ов.

9.  **Оптимизация и Управление Затратами:**
    *   Мониторинг стоимости LLM API.
    *   Улучшение дедупликации.
    *   Оптимизация промптов, возможное использование более дешевых моделей LLM для некоторых этапов (если качество позволяет).
    *   Кеширование.

10. **Тестирование и Оценка:**
    *   Тщательное тестирование каждого компонента.
    *   Оценка качества работы LLM-препроцессора и итоговых прогнозов.

**С чего Начинаем Прямо Сейчас:**

Давайте начнем с **Фазы 1, Шаг 1: Проектирование Промпта и Структуры Вывода для LLM-Препроцессора.**

Это самая творческая и одна из самых важных частей. Нам нужно четко определить:
*   Какой объем текста мы будем передавать LLM за один раз (целую статью или ее части)?
*   Как именно LLM должен разбивать текст на "умные чанки"? По каким критериям?
*   Какие конкретно метаданные мы хотим извлекать для каждого чанка (команды, игроки, тема, тональность, тип факта и т.д.)?
*   В каком точном JSON-формате LLM должен возвращать массив этих чанков и их метаданных?

Как только у нас будет четкое ТЗ для LLM, мы сможем приступить к модификации `llm_content_analyzer.py`.

Готовы погрузиться в проектирование промпта? Это определит успех всей последующей обработки контента.
